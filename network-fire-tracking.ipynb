{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e5dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a79adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.12.1+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c030fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    # constructor that takes in layer config\n",
    "    # this is only for starting initial layers, everything else should configure through attach\n",
    "    def __init__(self, input_nodes, activation=torch.nn.ReLU()):  \n",
    "        self.input_nodes = input_nodes\n",
    "        self.activation = activation\n",
    "        self.weights = []\n",
    "        self.fire_trackers = []\n",
    "        self.outputs = []\n",
    "        self.output_layers = []\n",
    "        \n",
    "    # takes an existing layer and attaches a newly created layer to the output of this one\n",
    "    # note that to make a complete layer it must have a fixed input and output shape, so this is non-mutating\n",
    "    def attach(self, layer_nodes):\n",
    "        new_layer = Layer(layer_nodes)\n",
    "        self.output_layers.append(new_layer)\n",
    "        \n",
    "        new_layer_weights = torch.zeros((self.input_nodes, layer_nodes,))\n",
    "        torch.nn.init.normal_(new_layer_weights, std=0.02)\n",
    "        self.weights.append(new_layer_weights)\n",
    "        self.fire_trackers.append(torch.zeros((self.input_nodes, layer_nodes,)))\n",
    "        # print(\"new layer created with shape\", new_layer_weights.shape)\n",
    "        \n",
    "        return new_layer\n",
    "        \n",
    "    # torch multiply input by layer weights\n",
    "    # store outputs on layer for training\n",
    "    # call eval on layers set as downstream to this one\n",
    "    # in future when recursive layers are supported, \n",
    "    # inputs may need to come with an id so that a layer doesn't track its upstream relationships as closely\n",
    "    def eval(self, input):\n",
    "        # print(\"intput and weight check\", input.shape, self.weights.shape)\n",
    "        self.input = input\n",
    "        self.outputs = []\n",
    "        for i, output_layer in enumerate(self.output_layers):\n",
    "            output = torch.matmul(input, self.weights[i])\n",
    "            output = self.activation(output)\n",
    "            self.outputs.append(output)\n",
    "            \n",
    "            # print(\"outputSheck\", output.shape)\n",
    "        \n",
    "        for i, output_layer in enumerate(self.output_layers):\n",
    "            # print(\"eval with\", self.outputs[i])\n",
    "            output_layer.eval(self.outputs[i])\n",
    "            \n",
    "    def train(self, reward, learning_rate, decay_rate):\n",
    "        # for now we will skip specialization based on whether output fires or not\n",
    "        # whole network is also skipping bias\n",
    "        # down the line we may want to add some constant input to enable similar behavior\n",
    "        for i, output in enumerate(self.outputs):\n",
    "            # print(\"train\", self.weights[i].shape, self.input.shape, output.shape)\n",
    "            weight_contributions = self.input * self.weights[i].transpose(0,1)\n",
    "            # print(\"conts\", weight_contributions.shape, self.fire_trackers[i].shape)\n",
    "            self.fire_trackers[i] += weight_contributions.transpose(0, 1)\n",
    "            self.fire_trackers[i] *= decay_rate\n",
    "            self.weights[i] += self.fire_trackers[i] * learning_rate * reward\n",
    "\n",
    "        return\n",
    "    \n",
    "    def mutate(self, std_dev):\n",
    "        new_layer = self.clone()\n",
    "        for weight in new_layer.weights:\n",
    "            mutation = torch.randn(weight.shape) * std_dev\n",
    "            weight += mutation\n",
    "            \n",
    "        return new_layer\n",
    "        \n",
    "    def clone(self):\n",
    "        new_layer = Layer(self.input_nodes)\n",
    "        \n",
    "        for weight in self.weights:\n",
    "            new_layer.weights.append(weight)\n",
    "            \n",
    "        for fire_tracker in self.fire_trackers:\n",
    "            new_layer.fire_trackers.append(torch.clone(fire_tracker))\n",
    "            \n",
    "        for output in self.outputs:\n",
    "            new_layer.outputs.append(torch.clone(output))\n",
    "            \n",
    "        # this wont work as the layers are not tensors\n",
    "        # we can clone here though so the clone is recursive\n",
    "        # this will break with recursive layers though?\n",
    "        for output_layer in self.output_layers:\n",
    "            new_layer.output_layers.append(output_layer)\n",
    "        \n",
    "        return new_layer\n",
    "    \n",
    "    def reset(self):\n",
    "        for i, fire_tracker in enumerate(self.fire_trackers):\n",
    "            self.fire_trackers[i] = torch.zeros(fire_tracker.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7895840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    # setup network\n",
    "    # create collections that will store inputs, layers, layer relationships?, and outputs\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    # take in list of input shapes, return a corresponding layer for each of them\n",
    "    def set_input(self, input_shapes):\n",
    "        self.inputs = []\n",
    "        for input_shape in input_shapes:\n",
    "            layer = Layer(input_shape, activation=torch.nn.Identity())\n",
    "            self.layers.append(layer)\n",
    "            self.inputs.append(layer)\n",
    "        \n",
    "        return self.inputs\n",
    "        \n",
    "    # take in a list of layers and set them as the output\n",
    "    # this way eval method can return whatever the outputs are configured to be\n",
    "    def set_output(self, output_layers):\n",
    "        self.output_layers = []\n",
    "        for output_layer in output_layers:\n",
    "            self.output_layers.append(output_layer)\n",
    "        \n",
    "    # take in inputs\n",
    "    # eval them on each of the set input layers\n",
    "    # layer eval pushes state to other layers at runtime\n",
    "    # so evaling layers should result in outputs being set\n",
    "    # outputs will be stored on layers anyways for weight adjustment\n",
    "    # so can just return outputs from registered output layers\n",
    "    def eval(self, inputs):\n",
    "        for i, input in enumerate(inputs):\n",
    "            self.inputs[i].eval(input)\n",
    "            \n",
    "        outputs = []\n",
    "        for output in self.output_layers:\n",
    "            # print(\"output\", output.input)\n",
    "            outputs.append(output.input)\n",
    "                \n",
    "            \n",
    "        return outputs\n",
    "        \n",
    "    # will adjust network weights based on current outputs stored on the layers\n",
    "    # should this take in learning rate so that something outside network can adjust this as needed?\n",
    "    def train(self, reward, learning_rate, decay_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.train(reward, learning_rate, decay_rate)\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def mutate(self, std_dev):\n",
    "        new_layer = self.clone()\n",
    "        return\n",
    "    \n",
    "    def reset(self):\n",
    "        return\n",
    "    \n",
    "    def clone(self):\n",
    "        for weight in self.inputs:\n",
    "            new_layer.weights.append(weight)\n",
    "            \n",
    "        for weight in self.output_layers:\n",
    "            new_layer.weights.append(weight)\n",
    "            \n",
    "        for weight in self.layers:\n",
    "            new_layer.weights.append(weight)\n",
    "            \n",
    "        return new_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea7f06d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.0000, 0.0088]])]\n"
     ]
    }
   ],
   "source": [
    "input = torch.zeros((1, 10,))\n",
    "input[0][0] = -1\n",
    "network = Network()\n",
    "input_layers = network.set_input((10,))\n",
    "hidden_layer1 = input_layers[0].attach(100)\n",
    "hidden_layer2 = hidden_layer1.attach(2)\n",
    "network.set_output((hidden_layer2,))\n",
    "\n",
    "output = network.eval((input,))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84bf9bdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 4]' is invalid for input of size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(observation)\n\u001b[0;32m---> 27\u001b[0m     new_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# print(\"input\", new_input)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     output \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39meval(new_input)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 4]' is invalid for input of size 6"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "torch.manual_seed(0)\n",
    "env = gym.make('Acrobot-v1')\n",
    "fitnesses = []\n",
    "\n",
    "network = Network()\n",
    "input_layers = network.set_input((6,))\n",
    "hidden_layer1 = input_layers[0].attach(64)\n",
    "hidden_layer2 = hidden_layer1.attach(64*2)\n",
    "hidden_layer3 = hidden_layer2.attach(3)\n",
    "network.set_output((hidden_layer3,))\n",
    "\n",
    "# attempt loop\n",
    "for i in range(100):\n",
    "    network.reset()\n",
    "    \n",
    "    # NOTE! the seed changing on the env is enabling exploration\n",
    "    # we NEED an exploration element to this algorithm\n",
    "    observation, info = env.reset(seed=42)#)# seed=42\n",
    "    fitness = 0\n",
    "    reward = 0\n",
    "    angle = 0\n",
    "    \n",
    "    # experience loop\n",
    "    for k in range(1000):\n",
    "        input = torch.tensor(observation)\n",
    "        new_input = input.reshape((1, 4))\n",
    "        # print(\"input\", new_input)\n",
    "        output = network.eval(new_input)\n",
    "        # print(\"output \", output)\n",
    "        action = torch.argmax(output[0]).item()\n",
    "        # print(\"action \", action)\n",
    "        \n",
    "        prev_reward = reward\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        fitness += reward\n",
    "\n",
    "        reward = -cos(theta1) - cos(theta2 + theta1)\n",
    "        \n",
    "        angle = cur_angle\n",
    "        \n",
    "        #if terminated or truncated:\n",
    "        #    reward = -10\n",
    "            \n",
    "        #print(reward, angle)\n",
    "        network.train(reward, .001, .9)\n",
    "\n",
    "        # have some kind of counter to enforce that we run each network through x lifetimes\n",
    "        # also add to some fitness counter as we go\n",
    "        if terminated or truncated:\n",
    "            # print(\"reset\", terminated, truncated)\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            break\n",
    "\n",
    "    fitnesses.append(fitness)\n",
    "    #print(\"Fitness: \", fitness)\n",
    "        \n",
    "    # take top n networks and make copies of them\n",
    "    # log fitnesses of top 10 + avg pop fitness\n",
    "    # consider using the child model where we also mutate to make the next generation\n",
    "    # should probably verify base algorithm first though\n",
    "plt.cla()\n",
    "plt.plot(fitnesses)\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7899a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "organisms = []\n",
    "for i in range(0, 100):\n",
    "    network = Network()\n",
    "    input_layers = network.set_input((4,))\n",
    "    hidden_layer1 = input_layers[0].attach(100)\n",
    "    hidden_layer2 = hidden_layer1.attach(2)\n",
    "    network.set_output((hidden_layer2,))\n",
    "    organisms.append(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1149ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval org  0 in generation  0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train() missing 3 required positional arguments: 'reward', 'learning_rate', and 'decay_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print(\"action \", action)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# make reward negative if terminated\u001b[39;00m\n\u001b[1;32m     25\u001b[0m fitness \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mTypeError\u001b[0m: train() missing 3 required positional arguments: 'reward', 'learning_rate', and 'decay_rate'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "observation, info = env.reset()# seed=42\n",
    "\n",
    "# generation loop\n",
    "for i in range(100):\n",
    "    fitnesses = []\n",
    "    for j, network in enumerate(organisms):\n",
    "        # eval loop\n",
    "        print(\"Eval org \", j, \"in generation \", i)\n",
    "        observation, info = env.reset()# seed=42\n",
    "        fitness = 0\n",
    "        for k in range(1000):\n",
    "            input = torch.tensor(observation)\n",
    "            new_input = input.reshape((1, 4))\n",
    "            # print(\"input\", new_input)\n",
    "            output = network.eval(new_input)\n",
    "            # print(\"output \", output)\n",
    "            action = torch.argmax(output[0]).item()\n",
    "            # print(\"action \", action)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            network.train()\n",
    "            \n",
    "            # make reward negative if terminated\n",
    "            fitness += reward\n",
    "            \n",
    "            # have some kind of counter to enforce that we run each network through x lifetimes\n",
    "            # also add to some fitness counter as we go\n",
    "            if terminated or truncated:\n",
    "                # print(\"reset\", terminated, truncated)\n",
    "                terminated = False\n",
    "                truncated = False\n",
    "                break\n",
    "                observation, info = env.reset()\n",
    "                network.reset()\n",
    "        \n",
    "        fitnesses.append((fitness, j))\n",
    "        print(\"Fitness: \", fitness)\n",
    "        \n",
    "    # take top n networks and make copies of them\n",
    "    # log fitnesses of top 10 + avg pop fitness\n",
    "    # consider using the child model where we also mutate to make the next generation\n",
    "    # should probably verify base algorithm first though\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56344b0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'display'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#mode='rgb_array')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(env\u001b[38;5;241m.\u001b[39mrender())\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m(plt\u001b[38;5;241m.\u001b[39mgcf())    \n\u001b[1;32m      9\u001b[0m     display\u001b[38;5;241m.\u001b[39mclear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'display'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(100):\n",
    "    #mode='rgb_array')\n",
    "    plt.imshow(env.render())\n",
    "    display.display(plt.gcf())    \n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    #print(observation)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"reset\", terminated, truncated)\n",
    "        break\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c533f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make set of random networks\n",
    "# let each run through env n times, training at each step and resetting prev fires when env is terminal\n",
    "# next take top m models and use those as population\n",
    "# repeat until fitness reaches goal / time runs out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
